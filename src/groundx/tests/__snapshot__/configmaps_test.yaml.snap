config-yaml.yaml:
  1: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-pxc-db-haproxy.eyelevel.svc.cluster.local
          rw_addr: db-cluster-pxc-db-haproxy.eyelevel.svc.cluster.local
          user: eyelevel
          password: password
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        ai:
          aws:
            search:
              baseURL: https://search-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: eyelevel
              password: R0otb_*t!kazs
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en
        engines:
          google/gemma-3-12b-it:
            engineID: "google/gemma-3-12b-it"
            maxInputTokens: 100000
            maxRequests: 4
            maxTokens: 2000
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 80
          serviceName: groundx

        init:
          ingestOnly: false
          mysql:
            user: root
            password: password
          search:
            password: R0otb_*t!kazs
            searchModel: all_access
            username: admin

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 80
          serviceName: layout-webhook

        kafka:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: eyelevel-stream
            topic: file-pre-process
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: eyelevel-stream
            topic: file-process
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: eyelevel-stream
            topic: file-summary
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: eyelevel-stream
            topic: file-summary
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: eyelevel-stream
            topic: file-update
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: eyelevel-stream
            topic: file-upload

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true

        summaryServer:
          baseURL: http://summary.eyelevel.svc.cluster.local
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: file.eyelevel.svc.cluster.local
          baseUrl: http://file.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://file.eyelevel.svc.cluster.local
          id: minio
          secret: minio123
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
layout-config-py.yaml:
  1: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="minio",
            accessSecret="minio123",
            annotationBase="layout/processed/",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsAddr="cache-metrics.eyelevel.svc.cluster.local",
            metricsPort=6379,
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="file.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadReplaceURL="http://file.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://file.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=["00000000-0000-0000-0000-000000000000", "00000000-0000-0000-0000-000000000000"],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
layout-gunicorn-conf-py.yaml:
  1: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "warn"
        workers = 2
        threads = 2
        timeout = 120

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
layout-supervisord-conf.yaml:
  1: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
config-models.yaml:
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
ranker-config-py.yaml:
  1: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            metricsAddr="cache-metrics.eyelevel.svc.cluster.local",
            metricsPort=6379,
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=["00000000-0000-0000-0000-000000000000", "00000000-0000-0000-0000-000000000000"],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
ranker-gunicorn-conf-py.yaml:
  1: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "warn"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
ranker-supervisord-conf.yaml:
  1: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-supervisord-conf-map
      namespace: eyelevel
summary-config-py.yaml:
  1: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.9,
            metricsAddr="cache-metrics.eyelevel.svc.cluster.local",
            metricsPort=6379,
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=["00000000-0000-0000-0000-000000000000", "00000000-0000-0000-0000-000000000000"],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
summary-gunicorn-conf-py.yaml:
  1: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "warn"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
summary-supervisord-conf.yaml:
  1: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-supervisord-conf-map
      namespace: eyelevel
init-database-file.yaml:
  1: |
    apiVersion: v1
    data:
      init-db.sql: |
        INSERT INTO partner_users (customer_username, customer_email, partner_username, status) values ('00000000-0000-0000-0000-000000000000', 'support@mycorp.net', '00000000-0000-0000-0000-000000000000', 'admin');

        INSERT INTO apikeys (apikey, name, username) values ('00000000-0000-0000-0000-000000000000', 'Admin Service Key', '00000000-0000-0000-0000-000000000000');

        INSERT INTO processors (name, description, username, ptype, priority) values
        ('Skip Generation', 'Does not add content to search database', 'system', 'skip-generate', 50), ('Save File', 'Saves output to a file', 'system', 'save-file', 50), ('System Document Layout', 'The default document layout processor', 'system', 'layout', 10), ('System Document Layout Mapping', 'The default document layout mapping processor', 'system', 'map', 15), ('Skip Document Layout', 'Does not process documents through layout pipeline', 'system', 'skip-layout', 50), ('Skip Document Mapping Layout', 'Does not map text through pipeline', 'system', 'skip-map', 50), ('Skip Molecule Summarizer', 'Does not summarize molecules in pipeline', 'system', 'skip-summarize', 50), ('System Combined Summarizer', 'The combined default summarizer', 'system', 'summarize', 20), ('System Section Summarizer', 'The default document and page summarizer', 'system', 'summarize-sections', 22), ('System Molecule Summarizer', 'The default molecule summarizer', 'system', 'summarize-chunks', 24), ('System Document Convert', 'The default document convert processor', 'system', 'convert', 7), ('Skip Document Convert', 'Does not convert documents', 'system', 'skip-convert', 50);

        INSERT INTO nlp_models (model_id, name, username, type, settings, indices) values (1, 'System Status - Search', '00000000-0000-0000-0000-000000000000', 'gpt3', '{"trainingFilesData":{"mainKit":{"fileId":"ey-mtr6hapxq7d94zigammwir6xz4","priority":20,"trainingFile":""}}}', 'prod-1');
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: init-database-file
      namespace: eyelevel
ldconfig-symlink.yaml:
  1: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
