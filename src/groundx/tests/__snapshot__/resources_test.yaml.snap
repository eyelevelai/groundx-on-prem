'aws: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: X.X.us-east-2.rds.amazonaws.com
          rw_addr: X.X.us-east-2.rds.amazonaws.com
          user: "eyelevel"
          password: "password"
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000001

        ai:
          aws:
            search:
              baseURL: https://X.us-east-2.es.amazonaws.com
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000001
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000001
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000001
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          mysql:
            user: "root"
            password: "password"
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000001

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-pre-process
          fileProcess:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-process
          fileSummaryDev:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileSummaryProd:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileUpdate:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-update
          fileUpload:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-upload

        rec:
          mysql: *mysql
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: X.s3.us-west-2.amazonaws.com
          baseUrl: https://X.s3.us-west-2.amazonaws.com
          bucket: eyelevel
          bucketUrl: https://X.s3.us-west-2.amazonaws.com
          service: s3
          ssl: true

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000001",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            layoutLogger="layout",
            layoutResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="X.s3.us-west-2.amazonaws.com",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="https://X.s3.us-west-2.amazonaws.com/",
            uploadSSL=True,
            uploadType="s3",
            uploadURL="https://X.s3.us-west-2.amazonaws.com/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000001",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            searchBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            searchLog="ranker",
            searchResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="ranker",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000001",
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="summary",
            summaryBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            summaryLog="summary",
            summaryResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000001",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'cloud: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-haproxy-replicas.eyelevel.svc.cluster.local
          rw_addr: db-cluster-haproxy.eyelevel.svc.cluster.local
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000000

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: X.s3.us-west-2.amazonaws.com
          baseUrl: https://X.s3.us-west-2.amazonaws.com
          bucket: eyelevel
          bucketUrl: https://X.s3.us-west-2.amazonaws.com
          id: "awsKey"
          region: us-west-2
          secret: "awsSecret"
          service: s3
          ssl: true

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        from groundx.extract import (
            AgentSettings,
            ContainerSettings,
            ContainerUploadSettings,
            GroundXSettings,
        )

        agent_settings = AgentSettings(
        )

        container_settings = ContainerSettings(
            broker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            broker_type="redis",
            cache_dir="/app/cache",
            callback_api_key="00000000-0000-0000-0000-000000000000",
            google_sheets_drive_id="google-drive-id",
            google_sheets_template_id="google-template-id",
            metrics_broker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="extract",
            upload=ContainerUploadSettings(
                base_domain="X.s3.us-west-2.amazonaws.com",
                bucket="eyelevel",
                region="us-west-2",
                ssl=True,
                type="s3",
                url="https://X.s3.us-west-2.amazonaws.com/",
                key="awsKey",
                secret="awsSecret",
            ),
            valid_api_keys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )

        groundx_settings = GroundXSettings(
          api_key="00000000-0000-0000-0000-000000000000",
          base_url="http://groundx.eyelevel.svc.cluster.local/api",
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=agent_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=download_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-download-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=save_agents_queue,celery
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="awsKey",
            accessSecret="awsSecret",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="X.s3.us-west-2.amazonaws.com",
            uploadBucket="eyelevel",
            uploadRegion="us-west-2",
            uploadReplaceURL="https://X.s3.us-west-2.amazonaws.com/",
            uploadSSL=True,
            uploadType="s3",
            uploadURL="https://X.s3.us-west-2.amazonaws.com/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  18: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  19: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  20: |
    apiVersion: v1
    data:
      GROUNDX_AGENT_API_KEY: b3BlbmFpLWFwaS1rZXk=
    kind: Secret
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-secret
      namespace: eyelevel
    type: Opaque
  21: |
    apiVersion: v1
    data:
      GCP_CREDENTIALS: PG5vIHZhbHVlPg==
    kind: Secret
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-secret
      namespace: eyelevel
    type: Opaque
  22: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  23: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  24: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'default: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-haproxy-replicas.eyelevel.svc.cluster.local
          rw_addr: db-cluster-haproxy.eyelevel.svc.cluster.local
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: minio.eyelevel.svc.cluster.local
          baseUrl: http://minio.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://minio.eyelevel.svc.cluster.local
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="minio.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=[
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'disabled: resources':
'empty: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-haproxy-replicas.eyelevel.svc.cluster.local
          rw_addr: db-cluster-haproxy.eyelevel.svc.cluster.local
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: minio.eyelevel.svc.cluster.local
          baseUrl: http://minio.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://minio.eyelevel.svc.cluster.local
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="minio.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=[
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'existing: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-ro.existing.com
          rw_addr: db-rw.existing.com
          user: "myuser"
          password: "mypass"
          database: mydb
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000000

        ai:
          aws:
            search:
              baseURL: https://search.existing.com
              index: other-1
              languages:
                - en
              username: "myuser"
              password: "mypass"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: openai-api-key
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            other-1:
              languages:
                - en

        engines:
          gpt-5-mini:
            engineID: "gpt-5-mini"
            service: openai
            reasoningEffort: low
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          mysql:
            user: "admin"
            password: "root"
          search:
            password: "admin"
            searchModel: all_access
            username: "root"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: my-pre-process
              threshold: 6
            - name: process
              target: my-process
              threshold: 9
            - name: upload
              target: my-upload
              threshold: 120
          session:
            addr: metrics-cache.existing.com:6379
            notCluster: true
            ssl: true
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: kafka.existing.com:9001
            groupId: my-kafka
            topic: my-pre-process
            type: kafka
          fileProcess:
            broker: other-kafka.existing.com:9002
            groupId: my-kafka
            topic: my-process
            type: kafka
          fileSummaryDev:
            broker: kafka.existing.com:9001
            groupId: my-summary
            topic: my-summary
            type: kafka
          fileSummaryProd:
            broker: kafka.existing.com:9001
            groupId: my-summary
            topic: my-summary
            type: kafka
          fileUpdate:
            key: awsKey
            region: us-east-2
            secret: awsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-update
          fileUpload:
            broker: kafka.existing.com:9001
            groupId: my-kafka
            topic: my-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.existing.com:6379
            notCluster: true
            ssl: true

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: file.existing.com:9000
          baseUrl: https://file.existing.com:9000
          bucket: mybucket
          bucketUrl: https://file.existing.com:9000
          id: "myuser"
          secret: "mypass"
          service: minio
          ssl: true

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="myuser",
            accessSecret="mypass",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="rediss://cache.existing.com:6379/0",
            layoutLogger="layout",
            layoutResultBroker="rediss://cache.existing.com:6379/0",
            metricsBroker="rediss://metrics-cache.existing.com:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="file.existing.com:9000",
            uploadBucket="mybucket",
            uploadRegion="",
            uploadReplaceURL="https://file.existing.com:9000/mybucket/",
            uploadSSL=True,
            uploadType="minio",
            uploadURL="https://file.existing.com:9000/mybucket/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="rediss://metrics-cache.existing.com:6379/0",
            searchBroker="rediss://cache.existing.com:6379/0",
            searchLog="ranker",
            searchResultBroker="rediss://cache.existing.com:6379/0",
            service="ranker",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
'extract: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: X.X.us-east-2.rds.amazonaws.com
          rw_addr: X.X.us-east-2.rds.amazonaws.com
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: true
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]
          extraPreDefaults:
          - processorID: 13
            type: extract
          extraPostDefaults:
            - processorID: 1
              type: skip-generate

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-pre-process
          fileProcess:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-process
          fileSummaryDev:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileSummaryProd:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileUpdate:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-update
          fileUpload:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-upload

        rec:
          mysql: *mysql
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: X.s3.us-west-2.amazonaws.com
          baseUrl: https://X.s3.us-west-2.amazonaws.com
          bucket: eyelevel
          bucketUrl: https://X.s3.us-west-2.amazonaws.com
          service: s3
          ssl: true

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        from groundx.extract import (
            AgentSettings,
            ContainerSettings,
            ContainerUploadSettings,
            GroundXSettings,
        )

        agent_settings = AgentSettings(
            api_base="http://summary-api.eyelevel.svc.cluster.local",
            model_id="google/gemma-3-4b-it",
        )

        container_settings = ContainerSettings(
            broker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            broker_type="redis",
            cache_dir="/app/cache",
            callback_api_key="",
            metrics_broker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="extract",
            upload=ContainerUploadSettings(
                base_domain="X.s3.us-west-2.amazonaws.com",
                bucket="eyelevel",
                region="",
                ssl=True,
                type="s3",
                url="https://X.s3.us-west-2.amazonaws.com/",
                key="",
                secret="",
            ),
            valid_api_keys=[
            ],
            workers=1,
        )

        groundx_settings = GroundXSettings(
          base_url="http://groundx.eyelevel.svc.cluster.local/api",
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=agent_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=download_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-download-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=save_agents_queue,celery
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            layoutLogger="layout",
            layoutResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="X.s3.us-west-2.amazonaws.com",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="https://X.s3.us-west-2.amazonaws.com/",
            uploadSSL=True,
            uploadType="s3",
            uploadURL="https://X.s3.us-west-2.amazonaws.com/",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      GCP_CREDENTIALS: PG5vIHZhbHVlPg==
    kind: Secret
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-secret
      namespace: eyelevel
    type: Opaque
  18: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="summary",
            summaryBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            summaryLog="summary",
            summaryResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  19: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  20: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'extract.ingest: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: X.X.us-east-2.rds.amazonaws.com
          rw_addr: X.X.us-east-2.rds.amazonaws.com
          user: "eyelevel"
          password: "password"
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000000

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: true
          mysql:
            user: "root"
            password: "password"
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]
          extraPreDefaults:
          - processorID: 13
            type: extract
          extraPostDefaults:
            - processorID: 1
              type: skip-generate

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            key: awsKey
            region: us-east-2
            secret: awsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-pre-process
          fileProcess:
            key: otherAwsKey
            region: us-east-2
            secret: otherAwsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-process
          fileSummaryDev:
            key: awsKey
            region: us-east-2
            secret: awsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileSummaryProd:
            key: awsKey
            region: us-east-2
            secret: awsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileUpdate:
            key: awsKey
            region: us-east-2
            secret: awsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-update
          fileUpload:
            key: awsKey
            region: us-east-2
            secret: awsSecret
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-upload

        rec:
          mysql: *mysql
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: X.s3.us-west-2.amazonaws.com
          baseUrl: https://X.s3.us-west-2.amazonaws.com
          bucket: eyelevel
          bucketUrl: https://X.s3.us-west-2.amazonaws.com
          id: "awsKey"
          region: us-west-2
          secret: "awsSecret"
          service: s3
          ssl: true

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        from groundx.extract import (
            AgentSettings,
            ContainerSettings,
            ContainerUploadSettings,
            GroundXSettings,
        )

        agent_settings = AgentSettings(
            api_base="http://summary-api.eyelevel.svc.cluster.local",
            model_id="google/gemma-3-4b-it",
        )

        container_settings = ContainerSettings(
            broker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            broker_type="redis",
            cache_dir="/app/cache",
            callback_api_key="00000000-0000-0000-0000-000000000000",
            metrics_broker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="extract",
            upload=ContainerUploadSettings(
                base_domain="X.s3.us-west-2.amazonaws.com",
                bucket="eyelevel",
                region="us-west-2",
                ssl=True,
                type="s3",
                url="https://X.s3.us-west-2.amazonaws.com/",
                key="awsKey",
                secret="awsSecret",
            ),
            valid_api_keys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )

        groundx_settings = GroundXSettings(
          api_key="00000000-0000-0000-0000-000000000000",
          base_url="http://groundx.eyelevel.svc.cluster.local/api",
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=agent_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=download_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-download-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=save_agents_queue,celery
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="awsKey",
            accessSecret="awsSecret",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            layoutLogger="layout",
            layoutResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="X.s3.us-west-2.amazonaws.com",
            uploadBucket="eyelevel",
            uploadRegion="us-west-2",
            uploadReplaceURL="https://X.s3.us-west-2.amazonaws.com/",
            uploadSSL=True,
            uploadType="s3",
            uploadURL="https://X.s3.us-west-2.amazonaws.com/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="summary",
            summaryBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            summaryLog="summary",
            summaryResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  18: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  19: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'extract.oai: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: X.X.us-east-2.rds.amazonaws.com
          rw_addr: X.X.us-east-2.rds.amazonaws.com
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          gpt-5-mini:
            engineID: "gpt-5-mini"
            service: openai-base64
            reasoningEffort: low
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: true
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]
          extraPreDefaults:
          - processorID: 13
            type: extract
          extraPostDefaults:
            - processorID: 1
              type: skip-generate

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-pre-process
          fileProcess:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-process
          fileSummaryDev:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileSummaryProd:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-summary
          fileUpdate:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-update
          fileUpload:
            type: sqs
            url: https://sqs.us-east-2.amazonaws.com/X/file-upload

        rec:
          mysql: *mysql
          session:
            addr: X.X.X.use2.cache.amazonaws.com:6379
            notCluster: false
            ssl: true

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: X.s3.us-west-2.amazonaws.com
          baseUrl: https://X.s3.us-west-2.amazonaws.com
          bucket: eyelevel
          bucketUrl: https://X.s3.us-west-2.amazonaws.com
          service: s3
          ssl: true

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        from groundx.extract import (
            AgentSettings,
            ContainerSettings,
            ContainerUploadSettings,
            GroundXSettings,
        )

        agent_settings = AgentSettings(
        )

        container_settings = ContainerSettings(
            broker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            broker_type="redis",
            cache_dir="/app/cache",
            callback_api_key="",
            metrics_broker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            service="extract",
            upload=ContainerUploadSettings(
                base_domain="X.s3.us-west-2.amazonaws.com",
                bucket="eyelevel",
                region="",
                ssl=True,
                type="s3",
                url="https://X.s3.us-west-2.amazonaws.com/",
                key="",
                secret="",
            ),
            valid_api_keys=[
            ],
            workers=1,
        )

        groundx_settings = GroundXSettings(
          base_url="http://groundx.eyelevel.svc.cluster.local/api",
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=agent_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=download_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-download-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=save_agents_queue,celery
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            layoutLogger="layout",
            layoutResultBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            metricsBroker="rediss://X.X.X.use2.cache.amazonaws.com:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="X.s3.us-west-2.amazonaws.com",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="https://X.s3.us-west-2.amazonaws.com/",
            uploadSSL=True,
            uploadType="s3",
            uploadURL="https://X.s3.us-west-2.amazonaws.com/",
            validAPIKeys=[
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      GCP_CREDENTIALS: PG5vIHZhbHVlPg==
    kind: Secret
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-secret
      namespace: eyelevel
    type: Opaque
'metadata: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat8",
            maxInputTokens = 50000,
            maxOutputTokens = 2000,
            maxRequests = 2,
            name = "google/gemma-3-8b-it",
            swapSpace = 10,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-haproxy-replicas.eyelevel.svc.cluster.local
          rw_addr: db-cluster-haproxy.eyelevel.svc.cluster.local
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            baseURL: http://myapp-api.eyelevel.svc.cluster.local
          layout:
            client:
              baseURL: http://myapp-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            baseURL: http://myapp-api.eyelevel.svc.cluster.local
            defaultKitId: 1
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-8b-it:
            engineID: "google/gemma-3-8b-it"
            service: eyelevel
            maxInputTokens: 50000
            maxRequests: 2
            requestLimit: 2
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://myapp.eyelevel.svc.cluster.local
          port: 8081
          serviceName: myapp

        init:
          ingestOnly: false
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: myapp
            - name: myapp-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: myapp-inference
              tokensPerMinute: 120000
            - name: myapp-api
              tokensPerMinute: 9600
            - name: myapp-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: myapp-myapp.eyelevel.svc.cluster.local:8082
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: myapp-correct
              target: my-queue
              threshold: 500
            - name: myapp-map
              target: my-queue
              threshold: 500
            - name: myapp-ocr
              target: my-queue
              threshold: 500
            - name: myapp-process
              target: my-queue
              threshold: 500
            - name: myapp-save
              target: my-queue
              threshold: 500

        owner:
          baseURL: http://myapp.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 2
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 2
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 2
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: myapp.eyelevel.svc.cluster.local:8082
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 5
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: minio.eyelevel.svc.cluster.local
          baseUrl: http://minio.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://minio.eyelevel.svc.cluster.local
          id: "minio"
          secret: "minio123"
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 2
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="minio",
            accessSecret="minio123",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="",
            deviceType="cpu",
            env="prod",
            includeLS=False,
            layoutBroker="redis://myapp.eyelevel.svc.cluster.local:8082/0",
            layoutLogger="myapp",
            layoutResultBroker="redis://myapp.eyelevel.svc.cluster.local:8082/0",
            metricsBroker="redis://myapp-myapp.eyelevel.svc.cluster.local:8082/0",
            minBatchSize=3,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="test-project",
            ocrType="google",
            podMemory="2Gi",
            queueType="kafka",
            service="myapp",
            uploadBase="layout/processed/",
            uploadBaseURL="minio.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=[
            ],
            workers=2,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 3
        threads = 3
        timeout = 10
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8081"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --queues=my-queue --concurrency=2
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      ldconfig_symlink.sh: |
        #!/bin/sh
        LDCONFIG_PATHS="/host-sbin/ldconfig /host-usr-sbin/ldconfig /host-bin/ldconfig /host-usr-bin/ldconfig"

        for path in $LDCONFIG_PATHS; do
          if [ -f "$path" ]; then
            echo "Found ldconfig at $path"
            cd "$(dirname "$path")" && ln -sf ldconfig /host-sbin/ldconfig.real
            break
          fi
        done

        if [ -L /host-sbin/ldconfig.real ]; then
          echo "Symbolic link created at /sbin/ldconfig.real"
        else
          echo "Failed to find ldconfig in the specified paths."
        fi
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ldconfig-symlink-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cpu",
            brokerType="redis",
            metricsBroker="redis://myapp-myapp.eyelevel.svc.cluster.local:8082/0",
            searchBroker="redis://myapp.eyelevel.svc.cluster.local:8082/0",
            searchLog="myapp",
            searchResultBroker="redis://myapp.eyelevel.svc.cluster.local:8082/0",
            service="myapp",
            validAPIKeys=[
            ],
            workers=4,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-config-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 4
        threads = 2
        timeout = 30
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8081"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-gunicorn-conf-py-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=2 --queues=my-queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=2 --queues=my-queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=2 --queues=my-queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=2 --queues=my-queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-inference-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cpu",
            deviceUtilize=0.37,
            brokerType="redis",
            metricsBroker="redis://myapp-myapp.eyelevel.svc.cluster.local:8082/0",
            service="myapp",
            summaryBroker="redis://myapp.eyelevel.svc.cluster.local:8082/0",
            summaryLog="myapp",
            summaryResultBroker="redis://myapp.eyelevel.svc.cluster.local:8082/0",
            validAPIKeys=[
            ],
            workers=2,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-config-py-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 5
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8081"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-gunicorn-conf-py-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=2 --queues=my-queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=2 --queues=my-queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: myapp-inference-supervisord-conf-map
      namespace: eyelevel
'minikube: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-haproxy-replicas.eyelevel.svc.cluster.local
          rw_addr: db-cluster-haproxy.eyelevel.svc.cluster.local
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000000

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: minio.eyelevel.svc.cluster.local
          baseUrl: http://minio.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://minio.eyelevel.svc.cluster.local
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="minio.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'openshift: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-cluster-haproxy-replicas.eyelevel.svc.cluster.local
          rw_addr: db-cluster-haproxy.eyelevel.svc.cluster.local
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000000

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 4
            requestLimit: 4
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx

        init:
          ingestOnly: false
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: minio.eyelevel.svc.cluster.local
          baseUrl: http://minio.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://minio.eyelevel.svc.cluster.local
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="",
            accessSecret="",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="minio.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.48,
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
'phoenix: resources':
  1: |
    apiVersion: v1
    data:
      config_models.py: |
        env = dict(
          summary = dict(
            dtype = "bfloat16",
            maxInputTokens = 100000,
            maxOutputTokens = 4096,
            maxRequests = 1,
            name = "google/gemma-3-4b-it",
            swapSpace = 16,
          ),
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-models-map
      namespace: eyelevel
  2: |
    apiVersion: v1
    data:
      config.yaml: |
        _mysql: &mysql
          ro_addr: db-ro.existing.com
          rw_addr: db-rw.existing.com
          user: "eyelevel"
          password: "password"
          database: eyelevel
          maxIdle: 5
          maxOpen: 10

        admin:
          apiKey: 00000000-0000-0000-0000-000000000000
          email: "support@mycorp.net"
          licenseKey: 00000000-0000-0000-0000-000000000000
          password: "password"
          username: 00000000-0000-0000-0000-000000000000

        ai:
          aws:
            search:
              baseURL: https://opensearch-cluster-master.eyelevel.svc.cluster.local:9200
              index: prod-1
              languages:
                - en
              username: "eyelevel"
              password: "R0otb_*t!kazs"
          extract:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://extract-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          eyelevelSearch:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://ranker-api.eyelevel.svc.cluster.local
          layout:
            client:
              apiKey: 00000000-0000-0000-0000-000000000000
              baseURL: http://layout-api.eyelevel.svc.cluster.local
              callbackURL: http://layout-webhook.eyelevel.svc.cluster.local
          openai:
            apiKey: 00000000-0000-0000-0000-000000000000
            baseURL: http://summary-api.eyelevel.svc.cluster.local
            defaultKitId: 0
          summaryType: summary
          searchIndexes:
            prod-1:
              languages:
                - en

        engines:
          google/gemma-3-4b-it:
            engineID: "google/gemma-3-4b-it"
            service: eyelevel
            maxInputTokens: 100000
            maxRequests: 48
            requestLimit: 48
            vision: true

        environment: prod

        groundxServer:
          baseURL: http://groundx-service.eyelevel.svc.cluster.local
          port: 8080
          serviceName: groundx-service

        init:
          ingestOnly: false
          mysql:
            user: "root"
            password: "password"
          search:
            password: "R0otb_*t!kazs"
            searchModel: all_access
            username: "admin"

        integrationTests:
          search:
            duration: 3660
            fileId: "ey-mtr6hapxq7d94zigammwir6xz4"
            modelId: 1

        layoutWebhookServer:
          baseURL: http://layout-webhook.eyelevel.svc.cluster.local
          port: 8080
          serviceName: layout-webhook

        metrics:
          active: true
          api:
            - name: groundx-service
            - name: layout-api
            - name: layout-webhook
          document:
            tokensPerMinute: 12500
          inference:
            - name: layout-inference
              tokensPerMinute: 120000
            - name: summary-api
              tokensPerMinute: 9600
            - name: summary-inference
              tokensPerMinute: 3200
            - name: summary-client
              tokensPerMinute: 9600
          page:
            tokensPerMinute: 500
          queue:
            - name: pre-process
              target: file-pre-process
              threshold: 6
            - name: process
              target: file-process
              threshold: 9
            - name: queue
              target: file-update
              threshold: 9
            - name: upload
              target: file-upload
              threshold: 120
          session:
            addr: cache-metrics.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false
          summaryRequest:
            tokensPerMinute: 625
          task:
            - name: layout-correct
              target: correct_queue
              threshold: 500
            - name: layout-map
              target: map_queue
              threshold: 500
            - name: layout-ocr
              target: ocr_queue
              threshold: 500
            - name: layout-process
              target: process_queue
              threshold: 500
            - name: layout-save
              target: save_queue,celery
              threshold: 500

        owner:
          baseURL: http://groundx-service.eyelevel.svc.cluster.local/api/v1
          name: on-prem
          type: all
          username: 00000000-0000-0000-0000-000000000000

        preProcessFileServer:
          baseURL: http://pre-process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: pre-process

        processFileServer:
          baseURL: http://process.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: process

        processors:
          convert: [11]
          extract: [13]
          layout: [3]
          map: [4]
          saveFile: [2]
          skipConvert: [12]
          skipGenerate: [1]
          skipLayout: [5]
          skipMap: [6]
          skipSummarize: [7]
          summarize: [8]
          summarizeChunks: [10]
          summarizeSections: [9]

        queueFileServer:
          baseURL: http://queue.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          pollTime: 1
          port: 8080
          serviceName: queue

        queues:
          filePreProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-pre-process
            topic: file-pre-process
            type: kafka
          fileProcess:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-process
            topic: file-process
            type: kafka
          fileSummaryDev:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileSummaryProd:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-summary
            topic: file-summary
            type: kafka
          fileUpdate:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-update
            topic: file-update
            type: kafka
          fileUpload:
            broker: stream-cluster-kafka-bootstrap.eyelevel.svc.cluster.local:9092
            groupId: file-upload
            topic: file-upload
            type: kafka

        rec:
          mysql: *mysql
          session:
            addr: cache.eyelevel.svc.cluster.local:6379
            notCluster: true
            ssl: false

        summaryServer:
          baseURL: http://summary-client.eyelevel.svc.cluster.local:8080
          maxConcurrent: 3
          port: 8080
          serviceName: summary-client

        upload:
          baseDomain: minio.eyelevel.svc.cluster.local
          baseUrl: http://minio.eyelevel.svc.cluster.local
          bucket: eyelevel
          bucketUrl: http://minio.eyelevel.svc.cluster.local
          id: "minio"
          secret: "minio123"
          service: minio
          ssl: false

        uploadFileServer:
          baseURL: http://upload.eyelevel.svc.cluster.local:8080
          maxConcurrent: 4
          port: 8080
          serviceName: upload
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: config-yaml-map
      namespace: eyelevel
  3: |
    apiVersion: v1
    data:
      config.py: |
        from groundx.extract import (
            AgentSettings,
            ContainerSettings,
            ContainerUploadSettings,
            GroundXSettings,
        )

        agent_settings = AgentSettings(
        )

        container_settings = ContainerSettings(
            broker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            broker_type="redis",
            cache_dir="/app/cache",
            callback_api_key="00000000-0000-0000-0000-000000000001",
            google_sheets_drive_id="abc",
            google_sheets_template_id="a123",
            metrics_broker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="extract",
            upload=ContainerUploadSettings(
                base_domain="X.s3.us-east-1.amazonaws.com",
                bucket="test",
                region="us-east-1",
                ssl=True,
                type="s3",
                url="https://X.s3.us-east-1.amazonaws.com/",
                key="key",
                secret="pass",
            ),
            valid_api_keys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000002",
            ],
            workers=1,
        )

        groundx_settings = GroundXSettings(
          api_key="00000000-0000-0000-0000-000000000001",
          base_url="https://api.groundx.ai/api",
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-config-py-map
      namespace: eyelevel
  4: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-gunicorn-conf-py-map
      namespace: eyelevel
  5: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=agent_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-supervisord-conf-map
      namespace: eyelevel
  6: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=download_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-download-supervisord-conf-map
      namespace: eyelevel
  7: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A celery_agents.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=save_agents_queue,celery
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-supervisord-conf-map
      namespace: eyelevel
  8: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            accessKey="minio",
            accessSecret="minio123",
            annotationBase="layout/processed/",
            brokerType="redis",
            cacheDir="/app/hf_models_cache",
            callbackAPIKey="00000000-0000-0000-0000-000000000000",
            deviceType="cuda",
            env="prod",
            includeLS=False,
            layoutBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            layoutLogger="layout",
            layoutResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            minBatchSize=40,
            ocrBase="layout/raw/",
            ocrCredentials="credentials.json",
            ocrProject="",
            ocrType="tesseract",
            podMemory="2Gi",
            queueType="kafka",
            service="layout",
            uploadBase="layout/processed/",
            uploadBaseURL="minio.eyelevel.svc.cluster.local",
            uploadBucket="eyelevel",
            uploadRegion="",
            uploadReplaceURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            uploadSSL=False,
            uploadType="minio",
            uploadURL="http://minio.eyelevel.svc.cluster.local/eyelevel/",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000002",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-config-py-map
      namespace: eyelevel
  9: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 2
        threads = 2
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "document_api_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-gunicorn-conf-py-map
      namespace: eyelevel
  10: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=correct_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-correct-supervisord-conf-map
      namespace: eyelevel
  11: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=layout_queue --concurrency=6
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /app/document_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /app/document_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-inference-supervisord-conf-map
      namespace: eyelevel
  12: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=map_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-map-supervisord-conf-map
      namespace: eyelevel
  13: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=ocr_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-ocr-supervisord-conf-map
      namespace: eyelevel
  14: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=process_queue --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-process-supervisord-conf-map
      namespace: eyelevel
  15: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A document.celery_process.app worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --queues=save_queue,celery --concurrency=1
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: layout-save-supervisord-conf-map
      namespace: eyelevel
  16: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            searchBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            searchLog="ranker",
            searchResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            service="ranker",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000002",
            ],
            workers=14,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-config-py-map
      namespace: eyelevel
  17: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 3
        timeout = 120
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "ranker_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-gunicorn-conf-py-map
      namespace: eyelevel
  18: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_2]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w2 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w2",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_3]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w3 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w3",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_4]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w4 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w4",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_5]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w5 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w5",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_6]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w6 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w6",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_7]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w7 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w7",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_8]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w8 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w8",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_9]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w9 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w9",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_10]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w10 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w10",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_11]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w11 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w11",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_12]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w12 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w12",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_13]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w13 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w13",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_worker_14]
        command=celery -A ranker.celery.appSearch worker -n %(ENV_POD_NAME)s-w14 --loglevel=INFO --concurrency=1 --queues=inference_queue
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w14",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: ranker-inference-supervisord-conf-map
      namespace: eyelevel
  19: |
    apiVersion: v1
    data:
      GROUNDX_AGENT_API_KEY: b3BlbmFpLWFwaS1rZXk=
    kind: Secret
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-agent-secret
      namespace: eyelevel
    type: Opaque
  20: |
    apiVersion: v1
    data:
      GCP_CREDENTIALS: PG5vIHZhbHVlPg==
    kind: Secret
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: extract-save-secret
      namespace: eyelevel
    type: Opaque
  21: |
    apiVersion: v1
    data:
      config.py: |
        env = dict(
            deviceType="cuda",
            deviceUtilize=0.95,
            brokerType="redis",
            metricsBroker="redis://cache-metrics.eyelevel.svc.cluster.local:6379/0",
            service="summary",
            summaryBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            summaryLog="summary",
            summaryResultBroker="redis://cache.eyelevel.svc.cluster.local:6379/0",
            validAPIKeys=[
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000000",
              "00000000-0000-0000-0000-000000000002",
            ],
            workers=1,
        )
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-config-py-map
      namespace: eyelevel
  22: |
    apiVersion: v1
    data:
      gunicorn_conf.py: |
        infolog = "-"
        accesslog = "-"
        errorlog = "-"

        loglevel = "info"
        workers = 1
        threads = 4
        timeout = 240
        timeout_keep_alive = 15

        worker_class = "uvicorn.workers.UvicornWorker"

        bind = "0.0.0.0:8080"

        wsgi_app = "summary_server:app"
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-gunicorn-conf-py-map
      namespace: eyelevel
  23: |
    apiVersion: v1
    data:
      supervisord.conf: |
        [supervisord]
        nodaemon=true
        logfile=/dev/null

        [program:celery_worker_1]
        command=celery -A summary.celery_inference.appSummary worker -n %(ENV_POD_NAME)s-w1 --loglevel=INFO --concurrency=1 --queues=summary_inference_queue --pool=solo
        environment=
          CELERY_WORKER_NAME="%(ENV_POD_NAME)s-w1",
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_monitor]
        command=python /workspace/summary_monitor.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0

        [program:celery_health]
        command=python /workspace/summary_health.py
        environment=
          LOCAL="0",
          PYTHONUNBUFFERED="1"
        autostart=true
        autorestart=true
        stdout_logfile=/dev/stdout
        stdout_logfile_maxbytes=0
        stderr_logfile=/dev/stderr
        stderr_logfile_maxbytes=0
    kind: ConfigMap
    metadata:
      labels:
        appVersion: 0.1.0
        chart: groundx-0.1.0
        heritage: Helm
        version: 0.1.0
      name: summary-inference-supervisord-conf-map
      namespace: eyelevel
