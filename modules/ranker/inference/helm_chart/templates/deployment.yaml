apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.service.name }}
  namespace: {{ .Values.service.namespace }}
  labels:
    app: {{ .Values.service.name }}
spec:
  replicas: {{ .Values.replicas.min }}
  selector:
    matchLabels:
      app: {{ .Values.service.name }}
  template:
    metadata:
      labels:
        app: {{ .Values.service.name }}
    spec:
      securityContext:
        runAsUser: {{ .Values.securityContext.runAsUser }}
        runAsGroup: {{ .Values.securityContext.runAsGroup }}
        fsGroup: {{ .Values.securityContext.fsGroup }}
      nodeSelector:
        node: "{{ .Values.nodeSelector.node }}"
      tolerations:
        - key: "node"
          value: "{{ .Values.nodeSelector.node }}"
          effect: "NoSchedule"
      initContainers:
        {{- if .Values.waitForDependencies }}
        - name: wait-for-cache
          image: {{ .Values.busybox.repository }}:{{ .Values.busybox.tag }}
          imagePullPolicy: "{{ .Values.busybox.pull }}"
          command: ["sh", "-c", "until nc -z {{ .Values.dependencies.cache }}; do echo waiting for cache; sleep 2; done"]
        {{- end }}
        {{- if .Values.createSymlink }}
        - name: create-symlink
          image: {{ .Values.busybox.repository }}:{{ .Values.busybox.tag }}
          imagePullPolicy: "{{ .Values.busybox.pull }}"
          command: ["/bin/sh", "-c", "--"]
          args: ["echo 'Running ldconfig_symlink.sh on this node'; /scripts/ldconfig_symlink.sh"]
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 10m
              memory: 50Mi
          volumeMounts:
            - name: {{ .Values.service.name }}
              mountPath: /scripts/ldconfig_symlink.sh
              subPath: ldconfig_symlink.sh
            - name: host-sbin
              mountPath: /host-sbin
            - name: host-usr-sbin
              mountPath: /host-usr-sbin
            - name: host-bin
              mountPath: /host-bin
            - name: host-usr-bin
              mountPath: /host-usr-bin
        {{- end }}
        - name: download-model
          image: {{ .Values.busybox.repository }}:{{ .Values.busybox.tag }}
          imagePullPolicy: "{{ .Values.busybox.pull }}"
          command:
            - /bin/sh
            - -c
            - |
              download_and_extract_model() {
                echo "Model downloading started..."

                touch /workspace/hf_models_cache/downloading

                echo "00 01 02" | tr ' ' '\n' | xargs -n 1 -P 5 -I {} sh -c \
                  'MAX_RETRIES=3; RETRIES=0; SUCCESS=0; PART={}; URL=https://upload.groundx.ai/ranker/model/{{ .Values.model }}/model.tar.gz.part.$PART; \
                  while [ $RETRIES -lt $MAX_RETRIES ]; do \
                    echo "Download [attempt $RETRIES] ranker.tar.gz.part.$PART"; \
                    wget -q -O /workspace/hf_models_cache/ranker.tar.gz.part.$PART $URL && { echo "Downloaded ranker.tar.gz.part.$PART successfully."; SUCCESS=1; break; }; \
                    echo "Failed to download $URL. Retrying..."; RETRIES=$((RETRIES + 1)); sleep 3; \
                  done; \
                  [ $SUCCESS -eq 0 ] && { echo "Failed to download $URL after $MAX_RETRIES attempts. Exiting."; rm /workspace/hf_models_cache/downloading; exit 1; }; \
                  echo "Unzipping ranker.tar.gz.part.$PART..."; tar -xzf /workspace/hf_models_cache/ranker.tar.gz.part.$PART -C /workspace/hf_models_cache/; echo "Unzipping ranker.tar.gz.part.$PART complete...";'

                rm /workspace/hf_models_cache/ranker.tar.*
                rm /workspace/hf_models_cache/downloading
                touch /workspace/hf_models_cache/complete
              }
              if [ ! -f /workspace/hf_models_cache/complete ]; then
                if [ ! -f /workspace/hf_models_cache/downloading ]; then
                  download_and_extract_model
                else
                  echo "Download in progress by another pod. Waiting..."
                  while [ -f /workspace/hf_models_cache/downloading ] || [ ! -f /workspace/hf_models_cache/complete ]; do
                    sleep $((3 + RANDOM % 2))
                  done

                  if [ ! -f /workspace/hf_models_cache/complete ]; then
                    download_and_extract_model
                  else
                    echo "Model cache ready."
                  fi
                fi
              else
                echo "Model cache already exists. Skipping download."
              fi

              echo "Model load done."
          securityContext:
            runAsUser: {{ .Values.securityContext.runAsUser }}
            runAsGroup: {{ .Values.securityContext.runAsGroup }}
            fsGroup: {{ .Values.securityContext.fsGroup }}
          volumeMounts:
            - name: model-volume
              mountPath: /workspace/hf_models_cache
      containers:
        - name: {{ .Values.service.name }}
          image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
          imagePullPolicy: "{{ .Values.image.pull }}"
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          workingDir: /workspace
          command:
            - /bin/sh
            - -c
            - |
              export PYTHONPATH=/workspace:$PYTHONPATH && supervisord -c /workspace/supervisord.conf
          securityContext:
            runAsUser: {{ .Values.securityContext.runAsUser }}
            runAsGroup: {{ .Values.securityContext.runAsGroup }}
            fsGroup: {{ .Values.securityContext.fsGroup }}
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - ps aux | grep 'ranker.celery.appSearch' | grep -v grep || exit 1
            initialDelaySeconds: 60
            failureThreshold: 10
            periodSeconds: 30
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - ps aux | grep 'ranker.celery.appSearch' | grep -v grep || exit 1
            initialDelaySeconds: 60
            periodSeconds: 30
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              {{- if .Values.cluster.setResources }}
              cpu: "{{ .Values.resources.requests.cpu }}"
              memory: "{{ .Values.resources.requests.memory }}"
              {{- end}}
              nvidia.com/gpu: 1
          volumeMounts:
            - name: config-models
              mountPath: /workspace/config_models.py
              subPath: config_models.py
            - name: config-volume
              mountPath: /workspace/config.py
              subPath: config.py
            - name: supervisord-volume
              mountPath: /workspace/supervisord.conf
              subPath: supervisord.conf
            - name: model-volume
              mountPath: /workspace/hf_models_cache
      volumes:
        - name: config-models
          configMap:
            name: config-models-map
        - name: config-volume
          configMap:
            name: ranker-config-py-map
        - name: supervisord-volume
          configMap:
            name: ranker-supervisord-16gb-conf-map
        {{- if .Values.createSymlink }}
        - name: {{ .Values.service.name }}
          configMap:
            name: ldconfig-symlink-map
            defaultMode: 0555
        - name: host-sbin
          hostPath:
            path: /sbin
            type: Directory
        - name: host-usr-sbin
          hostPath:
            path: /usr/sbin
            type: Directory
        - name: host-bin
          hostPath:
            path: /bin
            type: Directory
        - name: host-usr-bin
          hostPath:
            path: /usr/bin
            type: Directory
        {{- end }}
        - name: model-volume
          persistentVolumeClaim:
            claimName: {{ .Values.pv.name }}